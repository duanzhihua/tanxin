{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "\n",
    "from __init__ import *\n",
    "from src.ML.models import Models\n",
    "from src.utils import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_train = pd.read_csv(config.root_path + '/data/train_feature.tsv', sep='\\t')\n",
    "X_test = pd.read_csv(config.root_path + '/data/dev_feature.tsv', sep='\\t')\n",
    "y_train = pd.read_csv(config.root_path + '/data/train_label.tsv', sep='\\t', header=None)\n",
    "y_test = pd.read_csv(config.root_path + '/data/dev_label.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE, ADASYN\n",
    "# from imblearn.under_sampling import ClusterCentroids\n",
    "# # %time X_im, y_im = SMOTE().fit_resample(X_train, y_train)\n",
    "# # %time X_im, y_im = ADASYN(random_state=0).fit_resample(X_train, y_train)\n",
    "# # %time X_im_t, y_im_t = ADASYN(random_state=0).fit_resample(X_test, y_test)\n",
    "# %time X_im, y_im = ClusterCentroids(random_state=0).fit_resample(X_train, y_train)\n",
    "# %time X_im_t, y_im_t = ClusterCentroids(random_state=0).fit_resample(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from bayes_opt import BayesianOptimization\n",
    "\n",
    "# def bayes_parameter_opt_lgb(trn_data,\n",
    "#                             init_round=3,\n",
    "#                             opt_round=5,\n",
    "#                             n_folds=2,\n",
    "#                             random_seed=6,\n",
    "#                             n_estimators=1000,\n",
    "#                             learning_rate=0.05,\n",
    "#                             output_process=False):\n",
    "#     # parameters\n",
    "#     def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth,\n",
    "#                  lambda_l1, lambda_l2):\n",
    "#         params = {\n",
    "#             'application': 'multiclass',\n",
    "#             'num_iterations': n_estimators,\n",
    "#             'learning_rate': learning_rate,\n",
    "#             'early_stopping_round': 100,\n",
    "#             'num_class': len([x.strip() for x in open(config.root_path + '/data/class.txt', encoding='utf-8').readlines()]),\n",
    "#             'metric': 'multi_logloss'\n",
    "#         }\n",
    "#         params[\"num_leaves\"] = int(round(num_leaves))\n",
    "#         params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "#         params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "#         params['max_depth'] = int(round(max_depth))\n",
    "#         params['lambda_l1'] = max(lambda_l1, 0)\n",
    "#         params['lambda_l2'] = max(lambda_l2, 0)\n",
    "#         cv_result = lgb.cv(params,\n",
    "#                            trn_data,\n",
    "#                            nfold=n_folds,\n",
    "#                            seed=random_seed,\n",
    "#                            stratified=True,\n",
    "#                            verbose_eval=200)\n",
    "#         return max(cv_result['multi_logloss-mean'])\n",
    "#         # range\n",
    "\n",
    "#     lgbBO = BayesianOptimization(lgb_eval, {\n",
    "#         'num_leaves': (24, 31),\n",
    "#         'feature_fraction': (0.5, 0.9),\n",
    "#         'bagging_fraction': (0.6, 1),\n",
    "#         'max_depth': (2, 5),\n",
    "#         'lambda_l1': (1, 10),\n",
    "#         'lambda_l2': (1, 50),\n",
    "#     },\n",
    "#                                  random_state=0)\n",
    "#     # optimize\n",
    "#     lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "#     # output optimization process\n",
    "#     if output_process:\n",
    "#         lgbBO.points_to_csv(\"bayes_opt_result.csv\")\n",
    "#     # return best parameters\n",
    "#     print(lgbBO.max) \n",
    "#     return lgbBO.max\n",
    "\n",
    "# import lightgbm as lgb\n",
    "\n",
    "# trn_data = lgb.Dataset(data=X_im,\n",
    "#                        label=y_im,\n",
    "#                        free_raw_data=False)\n",
    "# tst_data = lgb.Dataset(data=X_im_t,\n",
    "#                        label=y_im_t,\n",
    "#                        free_raw_data=False)\n",
    "# param = bayes_parameter_opt_lgb(trn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param['params']['num_leaves'] = int(param['params']['num_leaves'])\n",
    "# param['params']['max_depth'] = int(param['params']['max_depth'])\n",
    "# param['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "model = lgb.LGBMClassifier(objective='multiclass',\n",
    "                            n_jobs=10,\n",
    "                            num_class=33,\n",
    "                            num_leaves=30,\n",
    "                            reg_alpha=10,\n",
    "                            reg_lambda=200,\n",
    "                            max_depth=3,\n",
    "                            learning_rate=0.05,\n",
    "                            n_estimators=2000,\n",
    "                            bagging_freq=1,\n",
    "                            bagging_fraction=0.9,\n",
    "                            feature_fraction=0.8,\n",
    "                            seed=1440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LGBMClassifier(bagging_fraction=0.7297396830471793, bagging_freq=1,\n               feature_fraction=0.9, lambda_l1=10.0, lambda_l2=19.0932427449065,\n               learning_rate=0.05, max_depth=5, n_estimators=2000, n_jobs=10,\n               num_class=33, num_leaves=24, objective='multiclass',\n               reg_alpha=10, reg_lambda=200, seed=1440)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "param = {'bagging_fraction': 0.7297396830471793,\n",
    " 'feature_fraction': 0.9,\n",
    " 'lambda_l1': 10.0,\n",
    " 'lambda_l2': 19.0932427449065,\n",
    " 'max_depth': 5,\n",
    " 'num_leaves': 24}\n",
    "model = model.set_params(**param)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "Test_predict_label = model.predict(X_test)\n",
    "Train_predict_label = model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(0.98795, 0.702, 0.702, 0.6950324432352321)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from src.utils.tools import get_score\n",
    "per, acc, recall, f1 = get_score(y_train, y_test,\n",
    "                                    Train_predict_label,\n",
    "                                    Test_predict_label)\n",
    "per, acc, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /Users/leonjiang/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\n2020-07-08 16:37:05,972 - /Users/leonjiang/Downloads/Project/bookClassification/src/word2vec/embedding.py[line:157] - INFO: load tfidf model\n2020-07-08 16:37:32,784 - /Users/leonjiang/Downloads/Project/bookClassification/src/word2vec/embedding.py[line:160] - INFO: load w2v model\n2020-07-08 16:37:46,100 - /Users/leonjiang/Downloads/Project/bookClassification/src/word2vec/embedding.py[line:164] - INFO: load fast model\n2020-07-08 16:37:54,340 - /Users/leonjiang/Downloads/Project/bookClassification/src/word2vec/embedding.py[line:168] - INFO: load lda model\nWARNING:root:failed to load state from /Users/leonjiang/Downloads/Project/bookClassification/model/embedding/lda.state: [Errno 2] No such file or directory: '/Users/leonjiang/Downloads/Project/bookClassification/model/embedding/lda.state'\n2020-07-08 16:37:54,801 - /Users/leonjiang/Downloads/Project/bookClassification/src/word2vec/embedding.py[line:171] - INFO: load autoencoder model\nINFO:/Users/leonjiang/Downloads/Project/bookClassification/logs/embedding.log:load autoencoder model\n"
    }
   ],
   "source": [
    "\n",
    "from __init__ import *\n",
    "from src.ML.models import Models\n",
    "from src.utils import config\n",
    "m = Models(train_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nDEBUG:jieba:Building prefix dict from the default dictionary ...\nLoading model from cache /var/folders/q2/2c_yc1ps73sflgwywyn7tmgc0000gp/T/jieba.cache\nDEBUG:jieba:Loading model from cache /var/folders/q2/2c_yc1ps73sflgwywyn7tmgc0000gp/T/jieba.cache\nLoading model cost 0.880 seconds.\nDEBUG:jieba:Loading model cost 0.880 seconds.\nPrefix dict has been built successfully.\nDEBUG:jieba:Prefix dict has been built successfully.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          title                                               desc  \\\n0  习近平谈治国理政 第一卷  2014年9月出版的《习近平谈治国理政》，近日由中央宣传部（国务院新闻办公室）会同中央文献研...   \n\n                                                text  \\\n0  习近平谈治国理政 第一卷2014年9月出版的《习近平谈治国理政》，近日由中央宣传部（国务院新...   \n\n                                            queryCut  \\\n0  [习近平, 谈, 治国, 理政,  , 第一卷, 2014, 年, 9, 月, 出版, 的,...   \n\n                                  queryCutRMStopWord  \n0  [习近平, 谈, 治国, 理政,  , 第一卷, 2014, 年, 9, 月, 出版, 的,...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>desc</th>\n      <th>text</th>\n      <th>queryCut</th>\n      <th>queryCutRMStopWord</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>习近平谈治国理政 第一卷</td>\n      <td>2014年9月出版的《习近平谈治国理政》，近日由中央宣传部（国务院新闻办公室）会同中央文献研...</td>\n      <td>习近平谈治国理政 第一卷2014年9月出版的《习近平谈治国理政》，近日由中央宣传部（国务院新...</td>\n      <td>[习近平, 谈, 治国, 理政,  , 第一卷, 2014, 年, 9, 月, 出版, 的,...</td>\n      <td>[习近平, 谈, 治国, 理政,  , 第一卷, 2014, 年, 9, 月, 出版, 的,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "from src.utils.tools import create_logger, wam, query_cut\n",
    "title = '习近平谈治国理政 第一卷'\n",
    "desc = '''2014年9月出版的《习近平谈治国理政》，近日由中央宣传部（国务院新闻办公室）会同中央文献研究室、中国外文局修订，改称《习近平谈治国理政》第一卷，由外文出版社面向海内外再版发行。\n",
    "　　《习近平谈治国理政》第一卷收入习近平总书记在党的十八大闭幕后至2014年6月13日期间的重要著作，共有讲话、谈话、演讲、答问、批示、贺信等79篇，分为18个专题。截至目前，该书以中、英、法、俄、阿、西、葡、德、日等24个语种、27个版本面向海内外出版发行，受到广泛关注和好评，为广大干部群众学习领会习近平新时代中国特色社会主义思想发挥了重要作用，为国际社会了解当代中国和中国共产党提供了重要文献。\n",
    "　　为帮助国内外读者系统了解掌握习近平新时代中国特色社会主义思想的精神实质和丰富内涵，2017年11月《习近平谈治国理政》第二卷出版发行。同时，应广大读者需要，对第一卷进行再版。\n",
    "　　《习近平谈治国理政》第一卷、第二卷是有机统一的整体，集中反映了习近平新时代中国特色社会主义思想的发展脉络和主要内容，生动记录了以习近平同志为核心的党中央团结带领全党全国各族人民在新时代坚持和发展中国特色社会主义的伟大实践，充分体现了中国共产党为推动构建人类命运共同体、促进人类和平与发展崇高事业贡献的中国智慧和中国方案，是国内外读者学习掌握习近平新时代中国特色社会主义思想和党的精神读本。\n",
    "　　当前，全党全社会正在深入学习《习近平谈治国理政》第二卷。第一卷的再版，对于推动习近平新时代中国特色社会主义思想和党的十九大精神深入人心，激励广大干部群众为决胜全面建成小康社会、开启全面建设社会主义现代化国家新征程、实现中华民族伟大复兴而奋斗，具有重大而深远的意义。'''\n",
    "df = pd.DataFrame([[title, desc]], columns=['title', 'desc'])\n",
    "df['text'] = df['title'] + df['desc']\n",
    "df[\"queryCut\"] = df[\"text\"].apply(query_cut)\n",
    "df[\"queryCutRMStopWord\"] = df[\"queryCut\"].apply(lambda x: [word for word in x if word not in m.ml_data.em.stopWords])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "transform w2v\n/Users/leonjiang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'generate_feature' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-5278d94f9d41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mtrain_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generate basic feature \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-5278d94f9d41>\u001b[0m in \u001b[0;36mget_embedding_feature\u001b[0;34m(data, tfidf, embedding_model)\u001b[0m\n\u001b[1;32m     44\u001b[0m     joblib.dump(w2v_label_embedding,\n\u001b[1;32m     45\u001b[0m                 config.root_path + '/data/w2v_label_embedding.pkl')\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_label_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w2v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtfidf_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_feature' is not defined"
     ]
    }
   ],
   "source": [
    "from src.utils.tools import (Grid_Train_model, bayes_parameter_opt_lgb,\n",
    "                             create_logger, formate_data, get_score)\n",
    "from src.utils.feature import (get_embedding_feature, get_img_embedding, generate_feature\n",
    "                               get_lda_features, get_pretrain_embedding,\n",
    "                               get_autoencoder_feature, get_basic_feature)\n",
    "import copy, json\n",
    "import numpy as np\n",
    "import joblib\n",
    "def get_embedding_feature(data, tfidf, embedding_model):\n",
    "    '''\n",
    "    @description: get_embedding_feature, tfidf, word2vec -> max/mean, word2vec n-gram(2, 3, 4) -> max/mean, label embedding->max/mean\n",
    "    @param {type}\n",
    "    mldata, input data set, mldata class instance\n",
    "    @return:\n",
    "    train_tfidf, tfidf of train data set\n",
    "    test_tfidf, tfidf of test data set\n",
    "    train, train data set\n",
    "    test, test data set\n",
    "    '''\n",
    "    data[\"queryCutRMStopWords\"] = data[\n",
    "        \"queryCutRMStopWord\"].apply(lambda x: \" \".join(x))\n",
    "    tfidf_data = pd.DataFrame(\n",
    "        tfidf.transform(\n",
    "            data[\"queryCutRMStopWords\"].tolist()).toarray())\n",
    "    tfidf_data.columns = [\n",
    "        'tfidf' + str(i) for i in range(tfidf_data.shape[1])\n",
    "    ]\n",
    "\n",
    "    print(\"transform w2v\")\n",
    "    data['w2v'] = data[\n",
    "        \"queryCutRMStopWord\"].apply(\n",
    "            lambda x: wam(x, embedding_model, aggregate=False))\n",
    "\n",
    "    train = copy.deepcopy(data)\n",
    "    labelNameToIndex = json.load(open(config.root_path +\n",
    "                                      '/data/label2id.json', encoding='utf-8'))\n",
    "    labelIndexToName = {v: k for k, v in labelNameToIndex.items()}\n",
    "    w2v_label_embedding = np.array([\n",
    "        embedding_model.wv.get_vector(labelIndexToName[key])\n",
    "        for key in labelIndexToName\n",
    "        if labelIndexToName[key] in embedding_model.wv.vocab.keys()\n",
    "    ])\n",
    "\n",
    "    joblib.dump(w2v_label_embedding,\n",
    "                config.root_path + '/data/w2v_label_embedding.pkl')\n",
    "    train = generate_feature(train, w2v_label_embedding, model_name='w2v')\n",
    "    return tfidf_data, train\n",
    "\n",
    "\n",
    "train_tfidf, df = get_embedding_feature(df, m.ml_data.em.tfidf, m.ml_data.em.w2v)\n",
    "\n",
    "print(\"generate basic feature \")\n",
    "df = get_basic_feature(df)\n",
    "\n",
    "print(\"generate modal feature \")\n",
    "df['cover'] = ''\n",
    "\n",
    "df['res_embedding'] = df.cover.progress_apply(\n",
    "    lambda x: get_img_embedding(x, m.res_model))\n",
    "\n",
    "df['resnext_embedding'] = df.cover.progress_apply(\n",
    "    lambda x: get_img_embedding(x, m.resnext_model))\n",
    "\n",
    "df['wide_embedding'] = df.cover.progress_apply(\n",
    "    lambda x: get_img_embedding(x, m.wide_model))\n",
    "\n",
    "print(\"generate bert feature \")\n",
    "df['bert_embedding'] = df.text.progress_apply(\n",
    "    lambda x: get_pretrain_embedding(x, m.bert_tonkenizer, m.bert\n",
    "                                        ))\n",
    "\n",
    "print(\"generate lda feature \")\n",
    "df['bow'] = df['queryCutRMStopWord'].apply(\n",
    "    lambda x: m.ml_data.em.lda.id2word.doc2bow(x))\n",
    "df['lda'] = list(\n",
    "    map(lambda doc: get_lda_features(m.ml_data.em.lda, doc),\n",
    "        df.bow))\n",
    "\n",
    "print(\"generate autoencoder feature \")\n",
    "df_ae = get_autoencoder_feature(\n",
    "    df,\n",
    "    m.ml_data.em.ae.max_features,\n",
    "    m.ml_data.em.ae.max_len,\n",
    "    m.ml_data.em.ae.model,\n",
    "    tokenizer=m.ml_data.em.ae.tokenizer)\n",
    "\n",
    "print(\"formate data\")\n",
    "df['labelIndex'] = 1\n",
    "df = formate_data(df, train_tfidf, df_ae)\n",
    "cols = [x for x in train.columns if str(x) not in ['labelIndex']]\n",
    "X_train = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          title                                               desc  \\\n0  习近平谈治国理政 第一卷  2014年9月出版的《习近平谈治国理政》，近日由中央宣传部（国务院新闻办公室）会同中央文献研...   \n\n                                                text  \\\n0  习近平谈治国理政 第一卷2014年9月出版的《习近平谈治国理政》，近日由中央宣传部（国务院新...   \n\n                                            queryCut  \\\n0  [习近平, 谈, 治国, 理政,  , 第一卷, 2014, 年, 9, 月, 出版, 的,...   \n\n                                  queryCutRMStopWord  \\\n0  [习近平, 谈, 治国, 理政,  , 第一卷, 2014, 年, 9, 月, 出版, 的,...   \n\n                                 queryCutRMStopWords  \\\n0  习近平 谈 治国 理政   第一卷 2014 年 9 月 出版 的 《 习近平 谈 治国 理...   \n\n                                                 w2v  length  capitals  \\\n0  [[1.8510973, -0.15333225, 0.06208436, 1.841589...     393         0   \n\n   caps_vs_length  ...  mean_word_len  punct_percent  cover  \\\n0             0.0  ...            1.0       5.597964          \n\n                                       res_embedding  \\\n0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n\n                                   resnext_embedding  \\\n0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n\n                                      wide_embedding  \\\n0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n\n                                      bert_embedding  \\\n0  [0.99481195, 0.96127987, 0.9924845, 0.08323939...   \n\n                                                 bow  \\\n0  [(4, 19), (5, 8), (6, 6), (7, 6), (10, 1), (13...   \n\n                                                 lda  labelIndex  \n0  [0.0005484532448463142, 0.530065655708313, 0.0...           1  \n\n[1 rows x 37 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>desc</th>\n      <th>text</th>\n      <th>queryCut</th>\n      <th>queryCutRMStopWord</th>\n      <th>queryCutRMStopWords</th>\n      <th>w2v</th>\n      <th>length</th>\n      <th>capitals</th>\n      <th>caps_vs_length</th>\n      <th>...</th>\n      <th>mean_word_len</th>\n      <th>punct_percent</th>\n      <th>cover</th>\n      <th>res_embedding</th>\n      <th>resnext_embedding</th>\n      <th>wide_embedding</th>\n      <th>bert_embedding</th>\n      <th>bow</th>\n      <th>lda</th>\n      <th>labelIndex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>习近平谈治国理政 第一卷</td>\n      <td>2014年9月出版的《习近平谈治国理政》，近日由中央宣传部（国务院新闻办公室）会同中央文献研...</td>\n      <td>习近平谈治国理政 第一卷2014年9月出版的《习近平谈治国理政》，近日由中央宣传部（国务院新...</td>\n      <td>[习近平, 谈, 治国, 理政,  , 第一卷, 2014, 年, 9, 月, 出版, 的,...</td>\n      <td>[习近平, 谈, 治国, 理政,  , 第一卷, 2014, 年, 9, 月, 出版, 的,...</td>\n      <td>习近平 谈 治国 理政   第一卷 2014 年 9 月 出版 的 《 习近平 谈 治国 理...</td>\n      <td>[[1.8510973, -0.15333225, 0.06208436, 1.841589...</td>\n      <td>393</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>5.597964</td>\n      <td></td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>[0.99481195, 0.96127987, 0.9924845, 0.08323939...</td>\n      <td>[(4, 19), (5, 8), (6, 6), (7, 6), (10, 1), (13...</td>\n      <td>[0.0005484532448463142, 0.530065655708313, 0.0...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 37 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bitbasecondac5d3152c1d0644d98b90e01e31307e7c",
   "display_name": "Python 3.6.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}